{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of Building Blocks\n",
    "## A tour of the Lego world in a data perspective\n",
    "\n",
    "\n",
    "### Executive Summary<a class=\"anchor\" id=\"Executive Summary\"></a>\n",
    "\n",
    "This is an ivestigation to explore the data related to Lego sets released since 1970. Lego started to manufacture its famous plastic bricks since the 1940s. From that time onwards numerous sets had been produced and sold worldwide. Lego has also accquired a massive fan base which share information and collections. This had opened up the prospect in perfoming analysis on Lego related data. This analysis can aid Lego collectors or retailers to make better decisions on the purchase or sale of Lego sets.\n",
    "\n",
    "The data is obtained by various means such as CSV files organised by individaul hobbyist. Web scraping and API enquiries were also used. EDA was performed and multiple visualisations were created to allow the readers to obtain a better understanding of the underlying data. Hypothesis testings were carried out to determine whether set with certain properties are different from the rest. \n",
    "\n",
    "One of the most important functions of data analysis is to make inferences or prediction using existing data. Hence predictive models were created and refined in order to predict the following:\n",
    "\n",
    "- Predict the Manufacturer Suggested Retail Price (MSRP)\n",
    "> MAE of 12.43 USD by \n",
    "- Distinguish desirable Lego sets\n",
    "> Accuracy of % by \n",
    "\n",
    "It can be said that the predictions were reasonably successful. During the invetigation it is determined that some features used can be labelled as latent features which are only generated after a set is released for long enough. Models based on these features might not be the most useful hence models excluding the latent features were also created and compared. It is found out that the error is similar without the latent features however the model becomes less stable which variance is higher for error score obtained by cross validation.\n",
    "\n",
    "An article published by The Gurdian has stated that Lego could be a better investment when compared to gold or stocks. Hence a model was built to try distinguishing those Lego sets that beats the market. This has resulted in a classification accuracy of 76% Using Extreme Gradient Boosting classifier.\n",
    "\n",
    "The above work has set up the foundation for future investigations which are also suggested at the end of the report. These work includes the gather and utilisation of Lego pieces data which should provide an even more refined and precise into the predictive models. Image recognition techniques can also be used to further expand the project into a build recommendation system based on the Lego pieces that the user already pocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "[Executive Summary](#Executive Summary) <br />\n",
    "[Background](#Background) <br />\n",
    "[Problem Statement and Success Criteria](#Problem Statement and Success Criteria) <br />\n",
    "[Gathering and Cleaning Data](#Gathering and Cleaning Data)<br />\n",
    "> [CSV and API](#CSV and API)<br />\n",
    "\n",
    "[EDA and Visualisations](#EDA and Visualisations) <br />\n",
    "> [Unique Features](#Unique Features)<br />\n",
    "> [Year](#Year)<br />\n",
    "> [Themes](#Themes)<br />\n",
    "> [Subthemes](#Subthemes)<br />\n",
    "\n",
    "[Hypothesis Testing](#Hypothesis Testing) <br />\n",
    "[MSRP Regression Analysis](#MSRP Regression Analysis) <br />\n",
    "[Desirability Analysis](#Desirability Analysis) <br />\n",
    "[Predicting Without Latent Features](#Predicting Without Latent Features) <br />\n",
    "[Beating The Market](#Beating The Market) <br />\n",
    "> [Web Scraping](#Web Scraping)<br />\n",
    "\n",
    "[Conclusion](#joke) <br />\n",
    "[Future Work](#Future Work) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In 1934, Lego was founded in Denmark by Ole Kirk Christiansen. Mr Christiansen started making wooden toys since 1932 and had produced different kinds of products. The predecessor of the today's famous plastic brick was launched in 1949 under the name \"Automatic Binding Bricks\". This was a risky move by Lego as it has spent  6.7% of its revenue on this machine alone. Luckily for Lego the investment paid off and Lego has since then progressed into one of the leading toy manufacturers in the world.\n",
    "\n",
    "Over the years Lego has improved and evolved many times to keep up with the market. The equivalents of the original bricks are still being manufactured but many more new themes have joined the lineup. For example Lego introduced the Duplo series was introduced in 1972. It is a series that are designed specially for children under the age of 5. These Duplo bricks features a significantly larger size which prevents them from chocking the target players. Another famous theme is the Techinic series which are models of much more complicated and sometimes machineries from the real world (for example BMW R 1200 GS Adventure Lego 42063). These advanced models are aim at more mature players. Modification to thes sets are also common which is partly due to the ease of integration of mechanical systems (such as pneumatic, electric and gears). Obtaining license from other products such as movies or comics has proved to be very successful for Lego as well. Amongst them the Star Wars theme is the most popular with a total of 585 sets released up to the date of writing. It can also be very collectable (expensive!?) with a Collector's  Millenium Falcon (Lego 10179) costing over 3000 GBP.\n",
    "\n",
    "Lego is also a big player in the robotics community. Due to the availability of parts and ease of constructions, Lego has been widely used in various robotic projects. In order to fulfill the demand from this sector, Lego released its own robotic package namely Mindstorms in 1998. Apart from the  various transducers and actuators, the series also features a programmable microchip called Intelligent Brick (similar to <a href='https://www.arduino.cc'>Arduino</a> and <a href='https://www.raspberrypi.org'>Raspberry Pi</a>). This makes Lego Mindstorms a very complete package for robotic hobbyists.\n",
    "\n",
    "There are many active communities formed by Lego enthusiasts. Some platforms are created specifically for trading such as <a href='http://www.bricklink.com'>Bricklink</a>. Other platforms are for Lego sets information such as <a href='http://www.brickset.com'>Brickset</a>. Some sites foucs more on Lego MOC which stands for \"My Own Creation\". MOC refers to non-official build ideas that are generated and shared amongst hobbyists. These information can be found in <a href='http://www.rebrickable.com'>Rebrickable</a>. Other great places to obtain infomation or generally discuss about Lego would be various groups and discussion forums such as <a href='https://www.reddit.com/r/lego/'>Reddit</a>.\n",
    "\n",
    "With the long history and a large fan base, Lego data has the potential in becoming an interesting field of study. One of the most popular investigations amongst enthusiast is the study carried out by <a href='http://www.realityprose.com/what-happened-with-lego/'>Reality Prose</a> where the relationship between Lego price and time was investigated. However it describes little about the pricing of Lego sets. It would be interesting for both retailers and hobbyist to find out what factors drive the price of Lego sets. It would also be useful for them to understand what might cause a Lego set to be desirable and collectable which in other words is a good investment. Therefore this project was carried out to tackle these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement and Success Criteria<a class=\"anchor\" id=\"Problem Statement and Success Criteria\"></a>\n",
    "\n",
    "\n",
    "Since the first creation of the famous plastic brick, Lego has expanded and developed many different lines and variations of bricks sets to provide more and better choice that suit different customers. It would be interesting for both retailers and hobbyist to find out what factors drive the price of Lego sets. It would also be useful for them to understand what might cause a Lego set to be desirable and collectable which in other words is a good investment. Some sources have stated that certain Lego sets are better investment than traditional investment products. Therefore it would also be interesting to find out and predict which Lego sets did/could likely beat the market. \n",
    "\n",
    "There are three major goals for this project. The first is to use various data and features to predict the MSRP of a Lego set. It is also important to determine the accuracy and methods for such predictions. This is because one can utilise either regression techniques or classification technique. The second goal would be to successfully come up with a metric to measure how collectable a Lego set is and be able to, again using the available data, predict which Lego sets are desirable for collectors. The third one will be to identify which Lego sets have beaten the market and to create a predictive model to try predicting sets that would beat the market in the future\n",
    "\n",
    "The success criteria for the project regarding to the first goal is to be able to\n",
    "- Predict price of a Lego set within 15% of its manufacturer suggested retail price (MSRP). Although this number is rather vague, it serves as a solid first step of the investigation. \n",
    "\n",
    "The success criteria for the project regarding to the second goal is to be able to\n",
    "- Define a metric to measure how desirable a Lego set is\n",
    "- Be able to classify Lego sets into more desirable/less desirable and beating the baseline accuracy (which depends on the definition of the metric)\n",
    "\n",
    "For both of the objectives above, the effect of ommiting latent features are to be examined\n",
    "\n",
    "The success criteria for the project regarding to the third goal is to be able to\n",
    "- Identify and validate Lego sets that have beaten the market\n",
    "- Be able to create a model that distinguishes Lego sets in their ability in beating the market and outperforms the baseline accuracy (which depends on the result of the identification step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering  and Cleaning Data<a class=\"anchor\" id=\"Gathering and Cleaning Data\"></a>\n",
    "\n",
    "The data for this project is gethered through 3 different channels. They are \n",
    "- CSV file from a Github Repository owned by a hobbyist <a href='https://github.com/seankross/lego/blob/master/data-tidy/legosets.csv'>seankross</a>\n",
    "- Application Programming Interface (API) provided by Brickset. The documentation can be found <a href='http://brickset.com/tools/webservices/v2'>here</a>\n",
    "- Web Scrapping to obtain information from Bricklink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV  and API<a class=\"anchor\" id=\"CSV and API\"></a>\n",
    "\n",
    "The CSV file is downloaded into the local storage and the a series of procedures were applied to the data in order to clean and prepare data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "lego = pd.read_csv('assets/datasets/legosets.csv')\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/CSV_info.png'>\n",
    "\n",
    "It is apparent that there are quite a lot of missing values in the lego data frame. Therefore the first approach is to try filling in the missing values by accessing the API provided by Brickset.com. In the same time some more useful information will be recorded. These information include.\n",
    "- Number of members owing this item\n",
    "- Number of members wanting this item\n",
    "- Rating of the item in Brickset.com\n",
    "- Theme Group of item\n",
    "- Number of ratings in Brickset.com for the item\n",
    "\n",
    "Also it is possible to see that there are a lot of missing values in the MSRP columns. Therefore it would be good it check if any of the can be filled.\n",
    "\n",
    "Upon the examinations of the lego data, it is discoverd that some item numbers are repeated. For example Lego has a series of products called Collectable Minifigures. These minifigures are selaed in foil packs and sold seperately. Customers cannot see through the foil packs and this adds an element of excitment similar to lucky draws. However this means the different minifigures have the same item numbers which is indistinguishable. Fortunately it is discovered that in the Image_URL column, the exact Brickset item number can be found. Regular expressions are used to extract such information which is then stored in a new column called 'query_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(lego)):\n",
    "    try:\n",
    "        lego.ix[i,'query_id'] = re.search(r'images/(.+-\\d+)',lego.ix[i,'Image_URL']).group(1)\n",
    "    except: \n",
    "        print 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 new columns are created before the data is gathered. This is because it is considered to be a more effective way to achieve data gathering. Normally one would use the .apply() method on the 'query_id' column to extract information and directly assign to a new variable/column. However since this is only viable for 1 column each time and there are 5 pieces of information that needs to be obtained, it is decided that it would be easier to modify existing columns. This approach reduces the number of get requests performed which speeds up the process and reduce the loading on the server. The considerations here being that it is not the most ethical to hit a server too frequently in a short period of time and by doing so might result in an I.P address ban which is undesirable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego['wanted'] = 0\n",
    "lego['owned'] = 0\n",
    "lego['rating'] = 0\n",
    "lego['theme_grp'] = 0\n",
    "lego['review_num'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the get_info function to obtain the information. The method is rather simple by getting the information of certain Lego sets using the Brickset API. Then various functions are applied to extract the desired information by apply regular expression searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_info(x):\n",
    "    print x\n",
    "    URL = 'http://brickset.com/api/v2.asmx/getSets?apiKey=yn4G-wvgQ-wNct&userHash=&query=&theme=&subtheme=&setNumber='+x+'&year=&owned=&wanted=&orderBy=&pageSize=&pageNumber=&userName='\n",
    "    response = requests.get(URL)\n",
    "    res = response.content\n",
    "    get_wanted(x,res)\n",
    "    get_owned(x,res)\n",
    "    get_theme_grp(x,res)\n",
    "    get_rating(x,res)\n",
    "    get_review_num(x,res)\n",
    "    get_usd(x,res)\n",
    "    get_gbp(x,res)\n",
    "    get_cad(x,res)\n",
    "    get_eur(x,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wanted(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<wantedByTotal>(.+)</wantedByTotal>',res).group(1)\n",
    "    except:\n",
    "        x = '1'\n",
    "    lego.ix[lego['query_id'] == q, 'wanted'] = x\n",
    "def get_owned(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<ownedByTotal>(.+)</ownedByTotal>',res).group(1)\n",
    "    except:\n",
    "        x = '1'\n",
    "    lego.ix[lego['query_id'] == q, 'owned'] = x\n",
    "def get_theme_grp(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<themeGroup>(.+)</themeGroup>',res).group(1)\n",
    "    except:\n",
    "        x = 'not_applicable'\n",
    "    lego.ix[lego['query_id'] == q, 'theme_grp'] = x\n",
    "def get_rating(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<rating>(.+)</rating>',res).group(1)\n",
    "    except:\n",
    "        x = '0'\n",
    "    lego.ix[lego['query_id'] == q, 'rating'] = x\n",
    "def get_review_num(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<reviewCount>(.+)</reviewCount>',res).group(1)\n",
    "    except:\n",
    "        x = '0'\n",
    "    lego.ix[lego['query_id'] == q, 'review_num'] = x\n",
    "def get_usd(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<USRetailPrice>(.+)</USRetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'USD_MSRP'] = x\n",
    "def get_gbp(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<UKRetailPrice>(.+)</UKRetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'GBP_MSRP'] = x\n",
    "def get_cad(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<CARetailPrice>(.+)</CARetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'CAD_MSRP'] = x\n",
    "def get_eur(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<EURetailPrice>(.+)</EURetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'EUR_MSRP'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.query_id.apply(get_info)\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/INFO_after_api.png'>\n",
    "\n",
    "The about info summary shows that there are still missing values in certain columns. However we now have the necessary columns to perform the analysis for the first 2 objectives. Since the API enquiry takes a rather large amount of time and computing power it is best to not performing it again. Hence a PostgreSQL database was created and the data stored in it as a table. Having gathered the required data the cleaning procedure would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "local_engine = create_engine('postgresql://localhost:5432')\n",
    "conn = local_engine.connect()\n",
    "conn.execute(\"commit\")\n",
    "conn.execute(\"CREATE DATABASE capstone\")\n",
    "conn.close()\n",
    "engine_capstone=create_engine('postgresql://localhost/capstone')\n",
    "lego.to_sql('lego',engine_capstone,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subtheme\n",
    "Lego sets can have more than 1 theme and this is denoted in the Subtheme column. However not everyset has a subtheme which is reflected in the data. However it would make analysis difficult if they are left to be 'NaN's hence they will be filled with the string 'No_Subtheme'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.Subtheme = lego.Subtheme.fillna('No_Subtheme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pieces\n",
    "\n",
    "There are a few instances with missing pieces information. It is because: \n",
    "- They are virtual products\n",
    "- Number of pieces are difficult to define\n",
    "- No information available\n",
    "\n",
    "And since these products make up a small part of the whole dataset it is decided they are to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego = lego[lego.Pieces.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minifigures\n",
    "\n",
    "Lego are renouned for their huge variaty of minifigures. However not every sets include them. For example the Technic series which are models of various machines usually have no minifigures included. It is believed that the 'NaN' values in the minifigures column are actually 0s. Therefore they will be filled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.Minifigures = lego.Minifigures.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSRP\n",
    "\n",
    "There are 4 currencies for MSRP figures. They are USD, GBP, CAD and EUR. However as a target for regression analysis, we do not need that many. Therefore all values will be changed to USD for a better comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.reset_index(drop=True,inplace=True)\n",
    "for i in range(len(lego)):\n",
    "    usd = lego.ix[i,'USD_MSRP']\n",
    "    gbp = lego.ix[i,'GBP_MSRP']\n",
    "    gbp_usd = 1.27\n",
    "    cad = lego.ix[i,'CAD_MSRP']\n",
    "    cad_usd = 0.75\n",
    "    eur = lego.ix[i,'EUR_MSRP']\n",
    "    eur_usd = 1.07\n",
    "    if usd != None:\n",
    "        pass\n",
    "    elif (gbp!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(gbp)*gbp_usd\n",
    "    elif (eur!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(eur)*eur_usd\n",
    "    elif (cad!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(cad)*cad_usd\n",
    "    else:\n",
    "        pass\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/after_fill_msrp.png'>\n",
    "\n",
    "By looping through the data frame most of this USD MSRP missing values are filled by converting from other currencies. However there are some that has no price information for every MSRP columns. Therefore they will have to be dropped. Together with them, the MSRP columns for GBP,CAD and EUR will also be dropped. The resulting dataframe would be stored in the database under the name lego_cleaned to avoid running through this cleaning procedure again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego = lego.drop(['GBP_MSRP','CAD_MSRP','EUR_MSRP'],axis=1)\n",
    "lego = lego.dropna()\n",
    "lego.to_sql('lego_cleaned',engine,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Visualisations<a class=\"anchor\" id=\"EDA and Visualisations\"></a>\n",
    "\n",
    "#### Unique Features<a class=\"anchor\" id=\"Unique Features\"></a>\n",
    "\n",
    "A brief description of the data can be made here. After the transfomations, there are 6060 rows and 20 columns left in the table. Some columns are unique and would add little value even if visulisations are applied. Those columns are \"Item_Number\", \"Name\", \"Image_URL\" and \"query_id\". \n",
    "\n",
    "#### Year<a class=\"anchor\" id=\"Year\"></a>\n",
    "\n",
    "The dataset contains records for Lego release fron 1975. It would be intresting to see the distribution amongst those years. It is also important to remember importing the required modules for the plotting tasks. Through out the report Matplotlib with Seaborn style plotting would be used as it provides the best felxibility in terms of plot s configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "yr = pd.pivot_table(lego, index=['Year'], values=['Name'], aggfunc='count')\n",
    "yr.plot(color='midnightblue', alpha=.7,ax=ax)\n",
    "ax.set(title='Number of Lego released by year', xlabel='Year', ylabel='Count',label='')\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/year_line.png'>\n",
    "\n",
    "The data set contains data ranges from 1971 to 2015. Although it is not the most updated (write up time of this experiment is Jan 2017), the amount of data (6060 entries at the moment) is sufficient for a proper analysis of trends and relationships. It can be seen that Lego has really rampped up their production through out the years. The most apparent drop in production would be around 2007 and 2008 when the financial crisis hit. It is worth noting that this plot does not show production number directly, it shows the variety of products released that year. However it is very likely for comapnies to expand their business in terms of production and variety in the same time when the economy is good. Therefore it would be a nice reference to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Themes<a class=\"anchor\" id=\"Themes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Theme.value_counts()[:20].plot(kind='barh', alpha=.7,ax=ax, color='midnightblue')\n",
    "ax.set(title='Top 20 Lego Theme', xlabel='Counts', ylabel='Theme')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lego_themes.png'>\n",
    "\n",
    "There are a total of 115 unique themes in the data set. It would be too much to display all of them. Therefore the above plot shows the top 20 themes in terms of numbers that were ever produced by Lego. It is worth noting that the top 20 themes have accounted for over 66% of all the data entries. This is an important finding as the large number of low frequency themes can introduce noise into the data which affects predictive modelling results.\n",
    "\n",
    "Having understand which themes have the most number of products, it would be good to see the number of years that the theme was actually in production. This is because naturally if the theme has been in production longer, it is more likely that it would have a higher number of products. If this is the case for every theme, the shape of the \"production years\" plot would be similar to the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theme_index = lego.Theme.value_counts()[:20].index.tolist()\n",
    "theme_index = [str(i) for i in theme_index]\n",
    "theme_pivot = pd.pivot_table(lego, index=['Theme'], values=['Year'], aggfunc=[np.min,np.max])\n",
    "theme_year = theme_pivot.loc[theme_index]\n",
    "c = theme_year.amax - theme_year.amin\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "c.plot(kind='barh', alpha=.7,ax=ax, color='crimson')\n",
    "ax.set(title='Years of production for the 20 top saling themes', xlabel='Years', ylabel='Theme')\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/theme_year.png'>\n",
    "\n",
    "It is immediately obvious that the shape of the plot is not as expected for every theme. Some themes have existed for long but the number of products are fewer than the other newer lines. For example the Space theme has been around for about 35 years but it has less products comparing to Collectable Minifigures. Duplo however has been the longest lasting theme yet as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subthemes<a class=\"anchor\" id=\"Subthemes\"></a>\n",
    "\n",
    "Many times a Lego set would have more than 1 theme. This is when the subtheme would come in to more specifically cataegorise the Lego set. For example the set Ferris Wheel 10247 (shown below), it has a theme of \"Advanced Model\" (persumably due to its complexity and large number of pieces) and a subtheme of Fairground. A similar approach can be taken in the analysis of sub themes.\n",
    "\n",
    "<img src='assets/pics/10247.jpg' style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Subtheme.value_counts()[1:21].plot(kind='barh', alpha=.7,ax=ax, color='midnightblue')\n",
    "ax.set(title='Top 20 Lego Sub Theme', xlabel='Counts', ylabel='Theme')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/subtheme.png'>\n",
    "\n",
    "From the above information we know that there are over 300 subthemes and each sub themes has less than 85 entries. Therefore each subtheme accounts for a very small part of the data set. The difference between sub themes are relatively small too. Therefore it is decided that the production years plot is not necessary. It is worth noting that it can be difficult to make sense with the subthemes alone. For example the subtheme that has the highest count is episode IV - VI. Without prior knowledge it would not be very interpretable (It turns out that this subtheme refers to Star Wars episode 4-6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pieces <a class=\"anchor\" id=\"Pieces\"></a>\n",
    "\n",
    "The maximum number of pieces in a set up to year 2015 is 5922 and the minimum is 0. It is not reasonable for a set/item to have 0 pieces so those items would receive further investigations. The shape of the plot clearly demonstrates that the distribution is heavily skewed positively. With a median peice of 83, we know that over half of the data entries have pieces less than 100. In order to investigate deeper, a zoomed in version of the distribution will be plotted with a range of 0 - 500 pieces. A log transformed plot is also showed for a better comparison. A symmetric distribution is shown in the log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Pieces.hist(color='midnightblue', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Number of pieces', xlabel='Number of pieces', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pieces.png', style='width:50%; float:left;'><img src='assets/pics/log_piece.png', style='width:50%;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego[lego.Pieces<500]['Pieces'].hist(color='midnightblue', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Number of pieces', xlabel='Number of pieces', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/zoom_piece.png'>\n",
    "\n",
    "Again we can see that the distribution is still positively skewd. Median value is 63 and 75 percentile lies at 171. It is also realised that starting at about 300 pieces the variations in set counts with number of pieces reduces significantly.\n",
    "\n",
    "We then move on to the investigation of sets that contains 0 pieces. It is discovered that there are 2 sets with 0 pieces which have item numbers of 71009 and 8299 respectively. Item Number 71009 describes a whole set of Lego minifigures. Referencing similar sets such as Item Number 8833 the average number of piece per figure is 7. Therefore the Pieces column will be change to 16 * 7 = 112. Item Number 8299 has 377 pieces where data is obatined from this <a href='https://www.bricklink.com/v2/catalog/catalogitem.page?S=8299-1#T=P'>Bricklink</a> page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minifigures\n",
    "\n",
    "Many sets contain minifigures to enhance the Lego experience. Minifigures can come with different bodys, legs and facial expressions. The different combinations add lots of interesting elements in the Lego range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Minifigures.hist(color='midnightblue', alpha=.7,ax=ax, bins=np.linspace(0,32,17))\n",
    "ax.set(title='Number of Minifigures in a set', xlabel='Number of Minifigures', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/minifig.png'>\n",
    "\n",
    "Again the plot show positive skewness for number of Minifigures. At least 75% of the sets have less than 2 minifigures. However the maxmium number of Minifugres is 32. This is also possible becuase Lego would sometimes release sets that are focused in minifigures because some collectors focuses their collections on Minifigures. An example would be the Community Figures Set (9348) shown below.\n",
    "\n",
    "<img src='assets/pics/9348.png' style='width:50%;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manufacturer Suggested Retail Price (MSRP)\n",
    "\n",
    "Similar to the plots for number of pieces, the price plots are also heavily skewed. Therefore a normal plot and zoomed in plot were created \n",
    "\n",
    "<img src='assets/pics/pirce.png' style='width:50%; float:left;'>\n",
    "<img src='assets/pics/price_zoom.png' style='width:50%;'>\n",
    "\n",
    "It is realised that most pieces are well below 20 USD. This is rather surprising as we can always see large sets in stores which costs a lot more. However 1 thing to bear in mind is that the prices in the data set has not been adjusted for inflation. Therefore inflation data is obtained from <a href='http://inflationdata.com/Inflation/Inflation_Rate/HistoricalInflation.aspx'>inflationdata.com</a> and adjustments were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inflation_lst = np.array([.043,.0327,.0616,.1103,.092,.0575,.065,.0762,.1122,.1358,.1035,.0616,.0322,.043,.0355,.0191,\n",
    "                 .0366,.0408,.0483,.0539,.0425,.0303,.0296,.0261,.0281,.0293,.0234,.0155,.0219,.0338,.0283,.0159,\n",
    "                 .0227,.0268,.0339,.0324,.0285,.0385,-0.0034,.0164,.0316,.0207,.0147,.0162])\n",
    "inf_rev = 1 - inflation_lst\n",
    "inf = [np.prod(inf_rev[i:]) for i in range(len(inflation_lst))]\n",
    "def adj_coef(x):\n",
    "    if x != 2015:\n",
    "        diff = x-1971\n",
    "        return inf[diff]\n",
    "    return 1\n",
    "lego['adjust_coef'] = lego['Year'].apply(adj_coef)\n",
    "lego.USD_MSRP = lego.USD_MSRP.astype(float)\n",
    "lego['adj_USD_MSRP'] = lego.USD_MSRP/lego.adjust_coef\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.adj_USD_MSRP.astype(float).hist(color='crimson', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Lego sets MSRP (USD)', xlabel='MSRP (USD)', ylabel='Count')\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.ix[lego.adj_USD_MSRP<100,'adj_USD_MSRP'].astype(float).hist(color='crimson', alpha=.7,ax=ax, bins=20)\n",
    "ax.set(title='Lego sets MSRP (USD)', xlabel='MSRP (USD)', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/adj_price.png' style='width:50%; float:left;'>\n",
    "<img src='assets/pics/adj_price_zoom.png' style='width:50%;'>\n",
    "\n",
    "\n",
    "Although the skewness is still there (which is expected), the distribution is now more 'even'. There are significantly more sets that ranges from 5 USD to 15 USD in terms of ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packaging\n",
    "\n",
    "Most Lego sets are packed as we see them - in boxes. However there are also other packaging methods. Although not as commonly seen as boxes, packaging using plastic bags are also popular. Those are called Polybags. In fact the inclusion of polybags in the data set might be one of the causes of skewness in price and pieces as polybags are usually samller in size (and hence a somewhat lower price is expected). Foilbags are used when Lego would like to hide the content in the bag from customers. The most popular example would be the Collectable Minifigure Series. It should also be noted that there are over 1500 instances with no packaging information.\n",
    "\n",
    "<img src='assets/pics/packaging.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability\n",
    "\n",
    "Most Lego sets are available for normal retail. However there are also other channels that Lego distributes its products. For example there are Legoland exclusive sets that customers can only purchase in Legoland. There are also limited editions that availability is significantly less than normal products. Similar to the Packaging feature, the availability feature has got over 1500 not-specified instances.\n",
    "\n",
    "<img src='assets/pics/availability.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanted and Owned\n",
    "\n",
    "In the website Brickset.com, registered users are able to indicate their interest in specific Lego sets by clicking a button saying that they want it. They can also record the ownership of their own Lego sets. Therefore for the same Lego item number, there are two figures showing the 'want' and 'own' respectively. The following plots demonstrate the distribution of such figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.wanted = lego.wanted.astype(float)\n",
    "lego.owned = lego.owned.astype(float)\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "lego.owned.hist(color='midnightblue', alpha=.7,ax=ax1, bins=20)\n",
    "ax1.set(title='Lego ownership in Brickset', xlabel='Number Owned', ylabel='Counts')\n",
    "lego.wanted.hist(color='midnightblue', alpha=.7,ax=ax2, bins=20)\n",
    "ax2.set(title='Lego wanted by members in Brickset', xlabel='Number Wanted', ylabel='Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ownwant.png'>\n",
    "\n",
    "From the plots above it can be seen that the distributions (in terms of shape) are very similar. Most pieces have very little votes. This could be due to the fact that not many sets are high profile sets and the majority receives very little attention. Therefore their votes on either categories are relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(lego.owned,lego.wanted, color='midnightblue', alpha=.7, label='data')\n",
    "ax1.plot(lego.owned,lego.owned, color='crimson', alpha=.7, label = 'Owned = Wanted')\n",
    "ax1.set(title='Owned vs Wanted', xlabel='Owned', ylabel='Wanted',xlim=[0,16000],ylim=[0,10000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ownscatter.png'>\n",
    "\n",
    "The above scatter plot describes the relationship between the 'wants' and 'owns' for various Lego sets. In order to assist the interpretation of the plot, a line showing want = own is also plotted. As expected we can see from the graph that most data points fall under the line. This means that most sets have more owns than wants. This is very common as the simple economic theory suggest that the rarer an items is the less supply it has; which in turns leads to a small number of total owners. The top 3 wanted sets, which all of them have over 8000 wanted votes, are shown below (2 of the are from Modular Building Series and the other is the Collector's Millenium Falcon): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/10182.jpg' style='width:30%; float:left;'>\n",
    "\n",
    "<img src='assets/pics/10185.png' style='width:38%; float:right;'>\n",
    "<img src='assets/pics/10179.jpg' style='width:28%; float:right;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the definition of classification labels\n",
    "\n",
    "As we can tell from the data set that there are no obvious labels to differentiate common sets from desirable sets, it is important to discuss how 'desirable-ness' should be measured. In the following summary the considered metrics would be proposed and its relevant pros and cons will be discussed.\n",
    "\n",
    "- Using the 'Wanted' number:\n",
    "This is the most straight forward metric that one can derived at. The biggest advantage in using this metric is that it is simple and easy to understand. The disadvantage of simply using 'wanted' is that it might not capture some information that is more complicated such as the number of people wanting a specific set. In other words if many people want a certain set but in the same time even more people own it. Does that still make the set desirable/collectable?\n",
    "\n",
    "\n",
    "- Using $Wanted/Owned$:\n",
    "Addressing the problem discussed above, one can use another label metric (denoted l): $$l = \\frac{Wanted}{Owned}$$This metric takes into account of set ownership as well. That is if a highly wanted set is also owned by many, l would not be high. This can in a way more accurately reflect the rarity of a Lego set. However this leads to another potential problem. The same 'l' can be achieved by different combinations of wanteds and owneds. For example having a 'l' metric of 3, there could be 3000 wanted and 1000 owned for set A and 3 wanted and 1 owned for set B. Common sense tells us that set A and set B are probably not equally desirable. Since set B receives less 'votes' in each category it might just means that set B is not very popular so hobbyist do not bother expressing their interest/ownership. This effect is not reflected in this 'l' metric.\n",
    "\n",
    "- Incorpration of certain 'distance' components:\n",
    "One way to mitigate the problem mentioned above is to include a distance component in the 'l' metric. There are two major ways that this could be achieved. The first one is to include a component in proportion to the data point's distance from the origin (point 0,0). This distance can effectively reflect the popularity of a data point. The greater the distance, the more vote a set has. This is because such distance is the square root of 'owned' squared + 'wanted' squared. \n",
    "\n",
    "Another distance component is the normal distance from the line wanted = owned. Since the data points can not have negative values, this distance is inheritedly small when it is near the origin. \n",
    "\n",
    "However it is tricky to incorprate these distance components in the 'l' metric. It is difficult to properly quantify their weighting and their effect on the metric. It is also more difficult for audiences to comprehend this metric.\n",
    "\n",
    "##### Colnclusion\n",
    "\n",
    "From the discussion above we know that there are advantages in itroducing a more complex lable to the dataset. However it is difficult to incorporate the metric properly. This is due to the fact that it is not poosible to easily integrate the distance component nor is the explantion to target audiences. Therefore for now the 'wanted' column wills serve as the 'desirable-ness' label with the top 10% defined as 'desirable'. However other labeling methods will also be investigated and improvement in this labeling system can take place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratings\n",
    "\n",
    "The shape of the rating distribution is rather unexpected. It can be seen that the ratings are concentrated in either end of the plot. It is suspected that most of the 0 rated items are actually items without ratings and the values were filled in during the data cleaning process. At the other end of the spectrum lies the review of the users. It is common for hoobyist to give reviews to those sets that they like but not to criticise the sets that are average. This could be the reason leading to this extreme situation.\n",
    "<img src='assets/pics/rating.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theme Group\n",
    "\n",
    "Theme group is just like \"Theme\" or \"Subtheme\" where sets are categorised. However theme group has less overall categories which allows users to compare better. It seems that theme group tends to indicate the 'purpose' of the set more than the actual background theme of the set. \n",
    "\n",
    "<img src='assets/pics/theme_grp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of reviews\n",
    "\n",
    "Again it shows that most sets have very small number of reviews which is less than or equal to 2. It is suspected that because most hobbyist would like to reviews sets that are special or popular - which accounts for a very small number of the total available data. The maximum review number is 97 and the minimum is 0. The median value is 2.\n",
    "\n",
    "<img src='assets/pics/review_num.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others\n",
    "\n",
    "##### Effect of inflation adjustment on prices\n",
    "\n",
    "The plots show the effect of inflation adjustment. Although the distribution shape stays rather similar, some patterns or evidence or thresholds disappear which were shown as horizontal lines in the non-adjusted plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.scatter(lego.Pieces,lego.adj_USD_MSRP, color='midnightblue', alpha=.7)\n",
    "ax1.set(title='adj_USD', xlabel='Number of pieces', ylabel='adj_USD')\n",
    "ax2.scatter(lego.Pieces,lego.USD_MSRP, color='midnightblue', alpha=.7)\n",
    "ax2.set(title='USD', xlabel='Number of pieces', ylabel='USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/adj_effect.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Price per pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppp = lego.adj_USD_MSRP/lego.Pieces\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ppp.hist(color='midnightblue', alpha=.7,ax=ax, bins=20)\n",
    "ax.set(title='Price per piece distribution', xlabel='Price per piece', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ppp.png'>\n",
    "\n",
    "From the above plot it can be seen that the price per pieces figures are mostly under 0.4 (0.37 is the 75 percentile). However some sets have a figure as high as over 207.30. In order to understand what is going on there a futher investigation was carried out.\n",
    "\n",
    "By looking at the data sorted by price per piece in decending order, it is possible conclude that the high 'price per piece' cases comes from mostly the Mindstorms theme. As mentioned in the background section, Mindstorms is a series of components made by Lego to allow users build complex mechanical systems. It is very widely used in robotics as we can see there are various sensors and transducers included in the theme. Due to the complexity the price of each piece would naturally be very high comparing to a plain plastic brick. Other unusually high priced pieces includes a play wall in the Duplo series which is like a table where you can build Lego on top. The following pivot table reinforces the statement made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(lego, index=['Theme'], values=['Price_per_piece']).sort_values(by='Price_per_piece',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ppp_sort.png'>\n",
    "\n",
    "The 2 top infintie values are likely caused by pieces being 0 for some entries for that category. This could generate error down the road but those will be dealt with in the modelling section. From the table it can be seen that after the errors, Mindstorms has and average of 37.4 USD per piece which is followed by Power Function (which also includes parts such as motors) in its series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Popularity and costs\n",
    "\n",
    "It would be normal to assume that when Lego recognises that some sets/themes tend to be more popular, it would give those sets a marked up price. Hence a 'wanted' vs 'Price_per_piece' scatter plot was generated to examine such effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(lego.wanted,lego.Price_per_piece,color='midnightblue', alpha=.7)\n",
    "ax.set(title='Popularity vs Price per pieces', xlabel='Number of wants', ylabel='Price per pieces', ylim=[0,50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/w_vs_ppp.png'>\n",
    "\n",
    "However from the plot there are no obvious trends demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of relationship between features\n",
    "\n",
    "In order to gain a deeper understanding in the relationship between features, a heatmap is created. The deeper the color of a square(either blue or red) the greater the realtionship there is. Red represents a positive crrelation and blue represents a negative correlation. From the plot we can see that there is a diagonal line runing thorguh the center which means variables are 100% correlated with themselves (obviously). However there are also other deep solid red squares within the map. These pairs are\n",
    "\n",
    "- Number of pieces and adjusted price\n",
    "- Number of 'wanted' votes and number of 'owned' vote\n",
    "- Number of 'wanted' votes and number of pieces\n",
    "\n",
    "These are all paris of variables that positively correlate with each other. In order to examine the actual distribution, a pairplot is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(lego.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/heat_map.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego_model = lego.drop(['Theme','Subtheme','Packaging','Availability','theme_grp'],axis=1)\n",
    "lego_model.rating = lego_model.rating.astype(float)\n",
    "lego_model.review_num = lego_model.review_num.astype(float)\n",
    "sns.pairplot(lego_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pairplot.png'>\n",
    "\n",
    "The pairplot shows various scatter plots and histograms to demonstrate relationship between features. In order to examine more on the findings that we obtained in the heat map, the relevant plots are analysed.\n",
    "\n",
    "We can see that all the combinations with higher correlations have a shape more like a funnel. Although it seems to be quite spreaded out, we have to remeber that we have more than 6000 data points and many of them might be overlayed by others. If the overlayed ones lie very close together and forms a linear line, the correlation coefficient can still be high. \n",
    "\n",
    "On the other hand others plots have either an irregular shape or shaped like a cluster that grows equally from the origin.\n",
    "\n",
    "There are other interesting insights that might or might not help in the modelling phase:\n",
    "\n",
    "\n",
    "- More recent sets are more popular both in terms of 'wanted' and 'owned'\n",
    "- Sets released in 2008 and 2009 seems to be reviewed the most which we can assume that they are more popular\n",
    "- Lego sets get bigger and bigger through out the year and topped at around 2008 and 2009 \n",
    "- However the most reviewed sets are not the largest sets. The most reviewed sets all have a number of pieces under 2000\n",
    "- There are usually Minifigures in sets that get reviewed a lot\n",
    "- Highly wanted sets are not necessarily expensive (vice versa)\n",
    "- A minimum rating of 4 is almost guaranteed if a set has at least 60 reviews\n",
    "- The number of reviews decreases as the sets get more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering analysis\n",
    "\n",
    "It is a common practise for data scientists to perform unsupervised learning (clustering) analysis during the EDA phase. The aim of clustering analysis is to organise similar data points together into groups (clusters). This can be especially helpful when the feature space is large and complicated where relationships cannot be discovered easily. Clustering analysis can also give an overview of the underlying data structure which can help a data sceintis to gauge his expectations in other models.\n",
    "\n",
    "Before performing the clustering analysis, it is important to understand what the algorithm is doing and make adjustments to the data accordingly. For many clustering algorithm, the Eucledian distances between data points are often used. This implies that feature scaling has to be carried out to prevent some features from dominating. In this example it would probably be the 'owned' feature which has the largest inter-feature variance.\n",
    "\n",
    "##### K-Means Clustering\n",
    "\n",
    "K-means clustering is an algorithm that requires the user defining the number of clusters. Therefore 4 different number of clusters are experimented and the results are shown below. It is obviously shown in the plots that there are no obvious clusters visible (which we can tell from the pairplot above). However it seems that K-means was able to differentiate groups of high-owned,low-wanted and high-wanted,low-owned (clusters in purple and yellow respectively in the 5-clusters plot). It can also distinguish relatively unpopular sets i.e. low-owned,low-wanted (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "lego_std = StandardScaler().fit_transform(lego)\n",
    "label_lst = []\n",
    "for n in range(2,6):\n",
    "    clus = KMeans(n_clusters=n,random_state=42).fit(lego_std)\n",
    "    label_lst.append(clus.labels_)\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax1.scatter(lego.wanted, lego.owned, c=label_lst[0], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax2.scatter(lego.wanted, lego.owned, c=label_lst[1], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax3.scatter(lego.wanted, lego.owned, c=label_lst[2], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax4.scatter(lego.wanted, lego.owned, c=label_lst[3], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax1.set(xlabel='Wants', ylabel='Owns', title='2 groups clustering using KMeans')\n",
    "ax2.set(xlabel='Wants', ylabel='Owns', title='3 groups clustering using KMeans')\n",
    "ax3.set(xlabel='Wants', ylabel='Owns', title='4 groups clustering using KMeans')\n",
    "ax4.set(xlabel='Wants', ylabel='Owns', title='5 groups clustering using KMeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/kmeans.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBScan\n",
    "\n",
    "Using DBScan, which is another type of clustering method, has yielded a very different result. Since number of clusters are not fixed in DBScan, the results are solely determined by the hyperparameters used in fitting the model. For DBScan it would be the 'eps' (the distance within which can nearby data form a cluster) and 'min_samples' (the minimum samples required for a cluster to form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "lego_std = StandardScaler().fit_transform(lego)\n",
    "clus = DBSCAN(min_samples=6, eps=1).fit(lego_std)\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(lego.wanted, lego.owned, c=clus.labels_, cmap=plt.get_cmap('rainbow'), alpha=.4)\n",
    "ax.set(xlabel='Wants', ylabel='Owns', title='Clustering using DBScan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/dbscan.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is yet anther method for performing clustering. It tries to connect the nearest data points together in each iteration until all the data points are connected. Once a group was formed, its 'location' is represented by the group's centroid. A Dendogram can be plotted to demonstrate how all the data connects however in our case there are too many data points which can not be displayed properly on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Z = linkage(lego, 'ward')\n",
    "c, coph_dists = cophenet(Z, pdist(lego))\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.title('Dendrogram',fontsize=50)\n",
    "plt.xlabel('Sets',fontsize=30)\n",
    "plt.ylabel('Distance',fontsize=30)\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=0.,  \n",
    "    leaf_font_size=18.,\n",
    ")\n",
    "plt.yticks(fontsize=25.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/dendogram.png'>\n",
    "\n",
    "The number of clusters produced in a hierachical clustering analysis depends on the decision of the analyst. The analyst can decide on a cut off point by looking at the 'Distance' axis. For example in our case a cut off of 60 would produce 5 clusters while a cut off at 125 would produce 2 clusters. Then clustering plots like the ones above can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_d = [125,100,80,60]\n",
    "clus_lst =[]\n",
    "for md in max_d:\n",
    "    clusters = fcluster(Z, md, criterion='distance')\n",
    "    clus_lst.append(clusters)\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax1.scatter(lego.wanted, lego.owned, c=clus_lst[0], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax2.scatter(lego.wanted, lego.owned, c=clus_lst[1], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax3.scatter(lego.wanted, lego.owned, c=clus_lst[2], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax4.scatter(lego.wanted, lego.owned, c=clus_lst[3], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax1.set(xlabel='Wants', ylabel='Owns', title='2 clusters @ distance = 125')\n",
    "ax2.set(xlabel='Wants', ylabel='Owns', title='3 clusters @ distance = 10')\n",
    "ax3.set(xlabel='Wants', ylabel='Owns', title='4 clusters @ distance = 80')\n",
    "ax4.set(xlabel='Wants', ylabel='Owns', title='5 clusters @ distance = 60')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/hier_clus.png'>\n",
    "\n",
    "The plots above show results that are reasonably similar to those produced by K-means. Similarly with the 4 cluster groups it seems that hierarchical clustering can can group data using number of wants and owns. The cluster boundaries are very similar to K-means as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "There are various Lego themes available in the market. It is also always an idea that some themes are more popular/expensive than the others. In this investigation, the aim is to verify this assumption with some of the better known themes such as Star Wars or Marvel Super Heros. Hypothesis testing will be carried out to compare the 'price per piece' of Lego sets to determine if Lego prices their sets differently. A subset of the complete dataset is used because the complete set contains many data that can seriously affect the test results. For example the Mindstorms series that we came across previously is usually very expensive as they consists of various sensors and motrs (or even a programmable chip). Therefore it is best to exclude those special cases to ensure we are comparing apples to apples (or bricks to bricks).\n",
    "\n",
    "Normally when hypothesis tests are mentioned, we think of t-tests or z-tests. These are the most common test seen in various situations. However in this case those tests are not appropriate because one of the prerequisite assumptions is not be met. This is namely the assumption normality of data which we found out during earlier analysis. From the histograms above we know that 'price per piece' for either of the groups are not normally distributed so t-test or z-test are not applicable. To solve this problem a Mann-Whitney test will be performed as this test is more robust to data with different distributions. One of the major reasons being that Mann-Whitney test utilises the value ranks of each data points instead of the mean in the test.\n",
    "\n",
    "4 groups are picked to be compared to the population. These are all common themes that we can see in toy stores. Like a t-test, a null hypothesis is defined. The null hypotheis suggests that there are no difference in price per piece between the population and the selected group. It would be reasonable to set a 95% confidence interval as this is just a normal hypothesis test which the result would not cause an immediate huge imapact (unlike those of mdicine testings). Therefore it would require a p-value of 0.05 or lower to reject the null hypothesis. Initial plots are shown to get an idea of what each group's distribution is like.\n",
    "\n",
    "<img src='assets/pics/4grp_dist.png'>\n",
    "\n",
    "It can be seen that for most groups the prices per piece are below 1. However the Star Wars theme has a few outliers that are very differnt. It might cause the test result to suggest it is different from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "lego['Price_per_piece'] = lego.adj_USD_MSRP/lego.Pieces\n",
    "relavent_lst = ['Duplo','Star Wars','City','Classic','Creator','The Simpsons','Scooby-Doo','Architecture','Elves',\n",
    "                'Friends','Ideas','Juniors','Minecraft','Ninjago','DC Comics Super Heroes','Marvel Super Heroes',\n",
    "                'Technic']\n",
    "condensed_lego = lego[lego['Theme'].isin(relavent_lst)]\n",
    "condensed_lego = condensed_lego[condensed_lego['Price_per_piece']!=0]\n",
    "\n",
    "\n",
    "population = condensed_lego['Price_per_piece']\n",
    "sw = condensed_lego[condensed_lego['Theme']=='Star Wars']['Price_per_piece']\n",
    "msh = condensed_lego[condensed_lego['Theme']=='Marvel Super Heroes']['Price_per_piece']\n",
    "ninja = condensed_lego[condensed_lego['Theme']=='Ninjago']['Price_per_piece']\n",
    "technic = condensed_lego[condensed_lego['Theme']=='Technic']['Price_per_piece']\n",
    "print 'Star Wars: ',mannwhitneyu(population,sw), len(sw)\n",
    "print 'Marvel Super Heroes: ',mannwhitneyu(population,msh), len(msh)\n",
    "print 'Ninjago: ',mannwhitneyu(population,ninja), len(ninja)\n",
    "print 'Technic: ',mannwhitneyu(population,technic), len(technic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/mw_wo.png'>\n",
    "\n",
    "From the results above it can be seen that the p-value for the Star Wars theme is extremely low! In fact it is the only theme where the null hypothesis can be rejected. Marvel Super Heroes almost made it there with a p-value of 0.06.\n",
    "However when we look at the earlier histograms, it can be realised that this effect might be caused by the outliers that exists in the group. Given group size of over 300 one might be able to justify a further test to see what it would be like without such outliers. Therefore a second test is conducted with all the sets that have a price per piece above 1 USD removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "population = condensed_lego[condensed_lego['Price_per_piece']<1]['Price_per_piece']\n",
    "sw = condensed_lego[(condensed_lego['Theme']=='Star Wars')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "msh = condensed_lego[(condensed_lego['Theme']=='Marvel Super Heroes')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "ninja = condensed_lego[(condensed_lego['Theme']=='Ninjago')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "technic = condensed_lego[(condensed_lego['Theme']=='Technic')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "print 'Star Wars: ',mannwhitneyu(population,sw), len(sw)\n",
    "print 'Marvel Super Heroes: ',mannwhitneyu(population,msh), len(msh)\n",
    "print 'Ninjago: ',mannwhitneyu(population,ninja), len(ninja)\n",
    "print 'Technic: ',mannwhitneyu(population,technic), len(technic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/mw_woo.png'>\n",
    "\n",
    "By looking at the group size from the results it can be seen that 7 and 8 sets were excluded from Star Wars and Technic respectively. This drastically changes the outcome where the null hypothesis can not be rejected for any of the cases (Star Wars at around 0.1 has the lowest p-value amongst them still). The data removed accouts for less than 4% in each group so it can be said that for the majority of the group the pricing is not that different. It is when we include the complete record we would have some big outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial regression analysis on Lego MSRP \n",
    "\n",
    "The first aim of the project is to predict Lego sets MSRP in USD. The success criteria is to get within 15% of the true price. From the EDA we can see that the price varies approximately linearly with some of the variables such as number of pieces. Therefore it would be logical to start of with models such as Linear Regressions and its regualrised counterparts. However before fitting any models, it would be necessary to refine the data into a format that the modeling module can take as input. It would also be great if some further plotting demonstrating the relationships between features. The required moduels are loaded into the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "lego_model = lego.drop(['Item_Number','Name','Image_URL','USD_MSRP','query_id','adjust_coef'], axis=1)\n",
    "theme = pd.get_dummies(lego.Theme, drop_first=True, prefix='theme')\n",
    "subtheme = pd.get_dummies(lego.Subtheme, drop_first=True, prefix='subtheme')\n",
    "packaging = pd.get_dummies(lego.Packaging, drop_first=True, prefix='packaging')\n",
    "availability = pd.get_dummies(lego.Availability, drop_first=True, prefix='availability')\n",
    "theme_grp = pd.get_dummies(lego.theme_grp, drop_first=True, prefix='theme_grp')\n",
    "lego_model = lego_model.drop(['Theme','Subtheme','Packaging','Availability','theme_grp'],axis=1)\n",
    "lego_model.rating = lego_model.rating.astype(float)\n",
    "lego_model.review_num = lego_model.review_num.astype(float)\n",
    "lego_w_dummies = pd.concat([lego_model,theme,subtheme,packaging,availability,theme_grp],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lego_model_info.png'>\n",
    "\n",
    "This is the data frame without the dummies attached. Note that for the regression analysis the whole data set is used instead of the subgroup created in the hypothesis testing section. Dummies (also known as one hot encoded features are created for the categorical variables. This is because due to the nature of categorical variables being non-ordinal, it is not possible to just encode them into numbers. By one hot encoding them (i.e. having n columns for n categories) each row would have a value '1' where it is relevant and '0' where it's not. However it should be mentioned that this method can possibly create a huge feature space which could be challenging in terms of computational requirements and the introduction of overfitting.\n",
    "\n",
    "The following steps define the features and targets. In order to prevent overfitting, a train-test split will be carried out. This operation divides the data into a training set and a testing set. This allows us to fit the model with the train data and test it with the unseen test data to avoid training bias where models test itself against seen data and report potentially unrelistic good results . Typically the train data would include the majority of instances in the data set to allow good model fit. In this case, it is decided that a 4:1 train-test split to be used. A random state is specified so that the splitting methodology can be fixed and repeated tests can be carried out with the same splits to provide fairer comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lego_w_dummies.drop('adj_USD_MSRP', axis=1)\n",
    "y = lego_w_dummies['adj_USD_MSRP']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "reg = LinearRegression()\n",
    "model = reg.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fitted the model and made a prediction, it is time to check the various metrics. There are three metrics that are commonly used in regression analysis namely mean squared error (MSE), mean absolute error (MAE) and R squared score (r2 score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'MSE: ',mean_squared_error(y_test,pred)\n",
    "print 'MAE: ',mean_absolute_error(y_test,pred)\n",
    "print 'R2: ', r2_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lr_rst.png'>\n",
    "\n",
    "In order to better visualise the model, the predicted price is plotted agains the actual price. In an ideal situation we sould observe a closely pcaked cluster which demonstrates a trend that represents that actual price equals to predicted price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax2.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax1.set(xlabel='True price',ylabel='Predicted price',title='Comparison between predicted price and actual price')\n",
    "ax2.set(xlabel='True price',ylabel='Predicted price',title='Comparison between predicted price and actual price',xlim=[0,600],ylim=[0,1200])\n",
    "ax2.plot(y_test,y_test,color='crimson',alpha=.7,label='True price line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lr_plt.png'>\n",
    "\n",
    "The plot on the left above is definetly not something that represents a good regression model. It can be observed that y axis has a scale of 1e8 which represents 100million. Therefore from the analysis we can see that a model is predicting a 50million price for one Lego set. This is not a reasonable price for any Lego set. There are other unreasonable outliers such as negative values too.\n",
    "\n",
    "However interestingly when we zoom in to limit true prices between 0-600 and predicted prices between 0-1200 (as shown in the right hand side plot), it can be seen that this regression is actually doing a fair job within this section. This can be shown with the 'True price line' which demonstrates what the plot would be like if the predictions were 100% accurate. We can see that in general the points in the graph lie relatively closely to the line.\n",
    "\n",
    "From the above information, it would be reasonable to assume that a linear model can perform well in this problem. However due to certain factors it can produce strange results for some cases. Overfitting would be a major contributor in this instability. \n",
    "\n",
    "In this model, there are 509 input features where all but 8 of them are categorical dummy variables. Intuitively it is reasonable to assume that not all the features are equally informative. On the other hand some features could be highly correlated. Either of those would affect the performance of the regression model.\n",
    "\n",
    "Therefore in order to try mitigating the effect of the described phenomenon, a Lasso regression and Ridge regression is performed as further investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "reg = Lasso()\n",
    "model = reg.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "score = cross_val_score(reg,X_train,y_train,cv=kf)\n",
    "maescore = cross_val_score(reg,X,y,cv=kf,scoring='neg_mean_absolute_error')\n",
    "msescore = cross_val_score(reg,X,y,cv=kf,scoring='neg_mean_squared_error')\n",
    "print 'MSE: ',mean_squared_error(y_test,pred)\n",
    "print 'MAE: ',mean_absolute_error(y_test,pred)\n",
    "print 'R2: ', r2_score(y_test,pred)\n",
    "print '10 fold CV average R2: ', np.mean(score)\n",
    "print '10 fold CV R2 standard deviation: ',np.std(score)\n",
    "print '10 fold CV average MAE: ', np.mean(maescore)\n",
    "print '10 fold CV MAE standard deviation: ', np.std(maescore)\n",
    "print '10 fold CV average MSE: ', np.mean(msescore)\n",
    "print '10 fold CV MSE standard deviation: ', np.std(msescore)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax1.set(xlabel='True price',\n",
    "        ylabel='Predicted price',\n",
    "        title='Comparison between predicted price and actual price',\n",
    "        xlim=[0,600],\n",
    "        ylim=[0,600])\n",
    "ax1.plot(y_test,y_test,color='crimson',alpha=.7,label='True price line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lasso_rst.png'>\n",
    "<img src='assets/pics/lasso.png'>\n",
    "\n",
    "A similar model using Ridge regression was created as well. The results are as follows\n",
    "\n",
    "<img src='assets/pics/ridge_rst.png'>\n",
    "<img src='assets/pics/ridge.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping<a class=\"anchor\" id=\"Web Scraping\"></a>\n",
    "\n",
    "In order to find out which Lego sets have beaten the market, it is necessary to obtain updated price figures for individual Lego sets. This information can not be obtained through the Brickset API and the API from Bricklink is difficult to use as it requires static IP address which is an inconvenience in my case as I have to constanly travel between home and school. Therefore a web scrapping solution was employed. The code for the scrapping is shown below and the explanation would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pickle\n",
    "from boto.s3.connection import S3Connection, Bucket, Key\n",
    "import urllib2\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "def get_box_sales(lst):\n",
    "    box_sales = {}\n",
    "    for i in lst:\n",
    "        print i\n",
    "        data = get_set_sales_info(i)\n",
    "        box_sales[i] = data       \n",
    "        if len(box_sales)%5 == 0:\n",
    "            print 'Collected ',len(box_sales),' items. ',len(lst)-len(box_sales), ' remains.'\n",
    "    return box_sales\n",
    "\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'w') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def change_format(lst):\n",
    "    new_lst = np.array([float(i.replace(',','')) for i in lst])\n",
    "    return new_lst\n",
    "\n",
    "def get_set_sales_info(query_id):\n",
    "    wd = webdriver.PhantomJS()\n",
    "    wd.get('http://www.bricklink.com/v2/catalog/catalogitem.page?S=%s#T=P' %(query_id))\n",
    "    WebDriverWait(wd, timeout=20).until(lambda x: x.find_element_by_tag_name('tr'))\n",
    "    page_source = wd.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    tables = soup.findAll(\"td\",valign=\"top\")\n",
    "    \n",
    "    try:\n",
    "        price_finds = re.findall(r'\\$(.+?\\.\\d+?)</td',str(tables[2]))\n",
    "        qty_finds = re.findall(r'>(\\d+)</td',str(tables[2]))\n",
    "        cur = 'USD'\n",
    "        if price_finds == []:\n",
    "            print 'try GBP'\n",
    "            price_finds = re.findall(r'GBP (.+?\\.\\d+?)</td',str(tables[2]))\n",
    "            qty_finds = re.findall(r'>(\\d+)</td',str(tables[2]))\n",
    "            cur = 'GBP'\n",
    "            \n",
    "    except:\n",
    "        print 'no data'\n",
    "\n",
    "    try:\n",
    "        price = change_format(price_finds)\n",
    "        qty = change_format(qty_finds)\n",
    "        mean = np.sum(price*qty)/np.sum(qty)\n",
    "        stdev = np.sqrt(np.sum(qty*((price-mean)**2))/qty-1)\n",
    "        print mean, stdev, cur\n",
    "        return (mean,stdev,sum(qty),cur)\n",
    "    \n",
    "    except:\n",
    "        print 'no record found'\n",
    "        pass\n",
    "    \n",
    "    print tables                    \n",
    "\n",
    "df_box = pd.read_csv('box.csv')\n",
    "query_lst = df_box.query_id.tolist()\n",
    "boxes = get_box_sales(query_lst)\n",
    "save_obj(boxes,'box_qty_price') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/Bricklink_example.png'>\n",
    "\n",
    "The above screen is captured to demonstrate the format of page that the scripts scrapes from. The Selenium module is used instead of a simple urllib/requests module. This is because the data in the tab is generated (or retrieved) by the JavaScript in the page which takes a certain time to load. Normal get methods can not compensate for this time lag and would result in a data not found. Selenium being a module used for web testing allows users to extract contents with delays or 'search until found' function which proves to be effective in this situation. However the down side of this script (overall) is that the scraping process is relatively low. In order to reduce the risk of broken connection during scraping, the task was carried out in an screened Amazon Web Services (AWS) EC2 instance. However due to the server location/browser cookies settings, the currency displayed by Bricklink in the EC2 instance is not stable which switches between USD and GBP. Hence a \"try/except\" and \"if\" statements were incorprated to capture the maximum amount of information available. \n",
    "\n",
    "The script is designed to get the current price of new sets. If there are no sets on sale currently, then that particular set would not be included in the analysis where current price is relevant. The price and quantity for each entries were recorded and the mean and sample standard deviation (using the Bessel's Correction) were calculated. The sample sizes and currency was also concluded should exchanging be applied.\n",
    "\n",
    "The results are stored in a dictionary with item query number (item number - revision) as the key. The completed dictionary is then packaged into a pickle item which can be transferred to the local disk or S3 storage space as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_current_data(query_id,data,status):\n",
    "    if query_id in price_dic:\n",
    "        cu = price_dic[query_id].loc[data,'current_'+status]\n",
    "        if cu==0:\n",
    "            cu = price_dic[query_id].loc[data,'average_'+status]\n",
    "        return cu\n",
    "    return np.nan\n",
    "\n",
    "# Scrapped data is in GBP so it is necessary to convert back into USD for comparison\n",
    "gbp_usd = 1.24\n",
    "box['current_new_price'] = box.query_id.apply(lambda x:merge_current_data(x,'Qty Avg Price','new'))*gbp_usd\n",
    "box.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Capstone_EDA.ipynb'>link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
